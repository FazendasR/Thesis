{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Move to Thesis directory (two levels up)\n",
    "os.chdir(os.path.abspath(os.path.join(\"..\", \"..\")))\n",
    "\n",
    "# Move to model/src if it exists\n",
    "model_dir = os.path.join(os.getcwd(), \"model\", \"src\")\n",
    "if os.path.exists(model_dir):\n",
    "    os.chdir(model_dir)\n",
    "\n",
    "print(\"Current Directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import os\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dictionary with all the program links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "dict_programs_links = pd.read_pickle(r\"../../data/Webscrapping/programs_links.pkl\")\n",
    "dict_programs_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "if len(dict_programs_links) > 1:\n",
    "    # Convert dictionary items to a list and get the second key-value pair\n",
    "    items = list(dict_programs_links.items())\n",
    "    second_url, second_filtered_links = items[1]\n",
    "    \n",
    "    # Print the second URL and filtered links\n",
    "    print(f\"Second URL: {second_url}\")\n",
    "    print(f\"Filtered links for the second URL: {second_filtered_links}\")\n",
    "elif dict_programs_links:\n",
    "    print(\"The dictionary does not have a second item.\")\n",
    "else:\n",
    "    print(\"The dictionary is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of the postgraduate and masters webpage design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postgraduate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Setup WebDriver\n",
    "driver_path = ChromeDriverManager().install()\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "first_url = second_filtered_links[0]\n",
    "\n",
    "# Open webpage\n",
    "driver.get(first_url)\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Extract the <main> element\n",
    "main_element = driver.find_element(By.TAG_NAME, \"main\")\n",
    "\n",
    "# Get the inner HTML of <main>\n",
    "main_html = main_element.get_attribute('outerHTML')\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Parse with BeautifulSoup\n",
    "soup = BeautifulSoup(main_html, \"html.parser\")\n",
    "\n",
    "\n",
    "# REMOVE THE BROCHURE AND COUNTRIES SECTIONS\n",
    "unwanted_section = soup.find(\"div\", class_=\"hero-article__info\")\n",
    "if unwanted_section:\n",
    "    unwanted_section.decompose()  \n",
    "\n",
    "# REMOVING THE STUDY PLAN (1), TEACHING STAFF (2), AND ACADEMIC CALENDER (3) SECTIONS\n",
    "articles = soup.find_all(\"article\", class_=\"content__article\")\n",
    "indices_to_remove = [0, 1, 2]  # 0-based index for 1th, 2th, and 3th articles\n",
    "for index in indices_to_remove:\n",
    "    if index < len(articles):\n",
    "        articles[index].decompose()\n",
    "    \n",
    "\n",
    "# Extract only the text, removing extra whitespace\n",
    "main_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "# Print extracted text\n",
    "print(main_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Setup WebDriver\n",
    "driver_path = ChromeDriverManager().install()\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "first_url = \"https://www.novaims.unl.pt/en/education/programs/postgraduate-programs-and-master-degree-programs/master-degree-program-in-data-science-and-advanced-analytics-with-a-specialization-in-data-science/\"\n",
    "\n",
    "# Open webpage\n",
    "driver.get(first_url)\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Extract the <main> element\n",
    "main_element = driver.find_element(By.TAG_NAME, \"main\")\n",
    "\n",
    "# Get the inner HTML of <main>\n",
    "main_html = main_element.get_attribute('outerHTML')\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Parse with BeautifulSoup\n",
    "soup = BeautifulSoup(main_html, \"html.parser\")\n",
    "\n",
    "# REMOVE THE BROCHURE AND COUNTRIES SECTIONS\n",
    "unwanted_section = soup.find(\"div\", class_=\"hero-article__info\")\n",
    "if unwanted_section:\n",
    "    unwanted_section.decompose()  \n",
    "\n",
    "# REMOVING THE STUDY PLAN (1), TEACHING STAFF (2), AND ACADEMIC CALENDER (3) SECTIONS\n",
    "articles = soup.find_all(\"article\", class_=\"content__article\")\n",
    "indices_to_remove = [0, 1, 2]  # 0-based index for 1th, 2th, and 3th articles\n",
    "for index in indices_to_remove:\n",
    "    if index < len(articles):\n",
    "        articles[index].decompose()\n",
    "\n",
    "# Extract only the text, removing extra whitespace\n",
    "main_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "# Print extracted text\n",
    "print(main_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Setup WebDriver\n",
    "driver_path = ChromeDriverManager().install()\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Target URL\n",
    "url = \"https://www.novaims.unl.pt/pt/ensino/cursos/licenciaturas/ciencia-de-dados/\"\n",
    "\n",
    "# Open webpage\n",
    "driver.get(url)\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Extract the <main> element\n",
    "main_element = driver.find_element(By.TAG_NAME, \"main\")\n",
    "\n",
    "# Get the inner HTML of <main>\n",
    "main_html = main_element.get_attribute('outerHTML')\n",
    "\n",
    "# Parse the original page with BeautifulSoup\n",
    "soup = BeautifulSoup(main_html, \"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "# Find specific links\n",
    "links_to_extract = [\n",
    "    \"https://www.novaims.unl.pt/en/education/programs/postgraduate-programs-and-master-degree-programs/master-degree-program-in-data-science-and-advanced-analytics-with-a-specialization-in-data-science/study-plan/\",\n",
    "    \"https://www.novaims.unl.pt/en/education/programs/postgraduate-programs-and-master-degree-programs/master-degree-program-in-data-science-and-advanced-analytics-with-a-specialization-in-data-science/teaching-staff/\"\n",
    "]\n",
    "\n",
    "# Extract text from each specified link and save to different files\n",
    "for link in links_to_extract:\n",
    "    # Navigate to the new URL\n",
    "    driver.get(link)\n",
    "    time.sleep(5)  # Wait for the new page to load\n",
    "\n",
    "    # Extract the new page's <main> element\n",
    "    new_main_element = driver.find_element(By.TAG_NAME, \"main\")\n",
    "    new_main_html = new_main_element.get_attribute('outerHTML')\n",
    "\n",
    "    # Parse the new page with BeautifulSoup\n",
    "    new_soup = BeautifulSoup(new_main_html, \"html.parser\")\n",
    "\n",
    "    # Extract only the text from the new page, removing extra whitespace\n",
    "    new_main_text = new_soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # Print extracted text from the new page\n",
    "    print(f\"\\nText from {link}:\")\n",
    "    print(new_main_text)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "- Both postgraduate and masters webpage design are very similar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for Postgraduate and Master Degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm ignoring at the moment schedules and timetables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Check if the dictionary is not empty\n",
    "if dict_programs_links:\n",
    "    # Get the first key (URL) and its value (filtered links)\n",
    "    first_url, first_filtered_links = next(iter(dict_programs_links.items()))\n",
    "    \n",
    "    # Print the first URL and filtered links\n",
    "    print(f\"First URL: {first_url}\")\n",
    "    print(f\"Filtered links for the first URL: {first_filtered_links}\")\n",
    "\n",
    "    # Extract the first three elements from the dictionary\n",
    "    first_three_programs = list(dict_programs_links.items())[:3]\n",
    "\n",
    "    # Iterate over the first three programs\n",
    "    for program_url, program_links in first_three_programs:\n",
    "        # Extract the name of the bachelor degree from the URL\n",
    "        posgrad_master_degree_name = program_url.split('/')[-2]\n",
    "\n",
    "        # Substitute the name in the links\n",
    "        links_to_extract = [\n",
    "            f\"https://www.novaims.unl.pt/en/education/programs/postgraduate-programs-and-master-degree-programs/{posgrad_master_degree_name}/program/#\",\n",
    "            f\"https://www.novaims.unl.pt/en/education/programs/postgraduate-programs-and-master-degree-programs/{posgrad_master_degree_name}/teaching-staff/#\"\n",
    "            \n",
    "        ]\n",
    "\n",
    "        # Print the substituted links\n",
    "        print(f\"Substituted links for {posgrad_master_degree_name}: {links_to_extract}\")\n",
    "\n",
    "        # Optionally, interact with the page after it loads\n",
    "else:\n",
    "    print(\"The dictionary is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Setup WebDriver\n",
    "driver_path = ChromeDriverManager().install()\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Directory for storing extracted text\n",
    "postgraduates_masters_dir = r\"..\\..\\data\\Webscrapping\\postgraduate_master_degrees\"\n",
    "os.makedirs(postgraduates_masters_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Sanitize the filename to remove any invalid characters.\"\"\"\n",
    "    # Replace invalid characters with underscores\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "\n",
    "def check_page_exists(url):\n",
    "    \"\"\"Returns True if the page contains a <main> element, otherwise False.\"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Allow page to load\n",
    "        driver.find_element(By.TAG_NAME, \"main\")  # Try finding <main>\n",
    "        return True  # If found, page exists\n",
    "    except NoSuchElementException:\n",
    "        return False  # <main> not found, assume page doesn't exist\n",
    "    except TimeoutException:\n",
    "        return False  # Page timeout, assume it doesn't exist\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking {url}: {e}\")\n",
    "        return False  # Catch-all for unexpected errors\n",
    "\n",
    "def extract_main_text(url, remove_teaching_staff, remove_study_plan):\n",
    "    \"\"\"Extracts the main content text from a given URL, adjusting section removals dynamically.\"\"\"\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Wait for page load\n",
    "\n",
    "    try:\n",
    "        main_element = driver.find_element(By.TAG_NAME, \"main\")\n",
    "        main_html = main_element.get_attribute('outerHTML')\n",
    "        soup = BeautifulSoup(main_html, \"html.parser\")\n",
    "\n",
    "        # Adjust which sections to remove\n",
    "        if not remove_study_plan and not remove_teaching_staff:\n",
    "            remove_indices = [0, 1, 2]  # Default: remove Study Plan (1), Teaching Staff (2), Academic Calendar (3)\n",
    "        elif not remove_study_plan:\n",
    "            remove_indices = [0, 2]  # Remove First Section (0) and Teaching Staff (2)\n",
    "        else:\n",
    "            remove_indices = [2]  # Remove only Teaching Staff (2)\n",
    "\n",
    "        # Remove unwanted sections\n",
    "        for selector in [\n",
    "            (\"div\", \"hero-article__info\", None),  # Remove Brochure and Country Sections\n",
    "            (\"article\", \"content__article\", remove_indices)\n",
    "        ]:\n",
    "            elements = soup.find_all(selector[0], class_=selector[1])\n",
    "            if selector[2]:  # If we have specific indices to remove\n",
    "                for index in selector[2]:\n",
    "                    if index < len(elements):\n",
    "                        elements[index].decompose()\n",
    "            elif elements:  # Remove the single found element\n",
    "                elements[0].decompose()\n",
    "\n",
    "        return soup.get_text(separator=\"\\n\", strip=True)\n",
    "    except NoSuchElementException:\n",
    "        print(f\"Error extracting text from {url}: <main> element not found.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Process second URL in `dict_programs_links`\n",
    "if len(dict_programs_links) > 1:\n",
    "    second_url, second_filtered_links = list(dict_programs_links.items())[1]\n",
    "    print(f\"Processing: {second_url}\")\n",
    "\n",
    "    for program_url in second_filtered_links:\n",
    "        posgrad_master_degree_name = program_url.split('/')[-2]  # Extract name from URL\n",
    "\n",
    "        # URLs to check\n",
    "        teaching_staff_url = f\"https://www.novaims.unl.pt/en/education/programs/postgraduate-programs-and-master-degree-programs/{posgrad_master_degree_name}/teaching-staff/#\"\n",
    "        study_plan_url = f\"https://www.novaims.unl.pt/en/education/programs/postgraduate-programs-and-master-degree-programs/{posgrad_master_degree_name}/study-plan/#\"\n",
    "\n",
    "        # Check if these pages exist\n",
    "        study_plan_exists = check_page_exists(study_plan_url)\n",
    "        teaching_staff_exists = check_page_exists(teaching_staff_url)\n",
    "\n",
    "        # Extract and save main course text\n",
    "        main_text = extract_main_text(program_url, not teaching_staff_exists, not study_plan_exists)\n",
    "        if main_text:\n",
    "            # Sanitize posgrad_master_degree_name to ensure it's a valid file name\n",
    "            sanitized_name = sanitize_filename(posgrad_master_degree_name)\n",
    "            \n",
    "            # Construct file path with sanitized name\n",
    "            file_path = os.path.join(postgraduates_masters_dir, f\"{sanitized_name}_main_course_extracted_text.txt\")\n",
    "            \n",
    "            # Write extracted text to the file\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(f\"Text from {program_url}:\\n{main_text}\\n\\n\")\n",
    "\n",
    "        # Extract and save text from study-plan & teaching-staff pages\n",
    "        for url, exists in [(study_plan_url, study_plan_exists), (teaching_staff_url, teaching_staff_exists)]:\n",
    "            if exists:\n",
    "                extracted_text = extract_main_text(url, not teaching_staff_exists, not study_plan_exists)\n",
    "                if extracted_text:\n",
    "                    # Sanitize posgrad_master_degree_name and page-specific name\n",
    "                    sanitized_name = sanitize_filename(posgrad_master_degree_name)\n",
    "                    page_name = url.split('/')[-2]  # Extract page name (study-plan, teaching-staff)\n",
    "                    file_name = os.path.join(postgraduates_masters_dir, f\"{sanitized_name}_{page_name}_extracted_text.txt\")\n",
    "                    \n",
    "                    # Write extracted text to the file\n",
    "                    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "                        file.write(f\"Text from {url}:\\n{extracted_text}\\n\\n\")\n",
    "\n",
    "    print(\"Text extraction completed and saved.\")\n",
    "else:\n",
    "    print(\"The dictionary is empty.\")\n",
    "\n",
    "# Close WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Extract course names from dict_programs_links\n",
    "if len(dict_programs_links) > 1:\n",
    "    second_url, second_filtered_links = list(dict_programs_links.items())[1]  # Get second key-value pair\n",
    "    course_names = [url.split('/')[-2] for url in second_filtered_links]  # Extract course names from URLs\n",
    "else:\n",
    "    print(\"Dictionary does not have enough data.\")\n",
    "    course_names = []\n",
    "\n",
    "# Expected file templates for each course\n",
    "expected_file_templates = [\n",
    "    \"{}_main_course_extracted_text.txt\",\n",
    "    \"{}_study-plan_extracted_text.txt\",\n",
    "    \"{}_teaching-staff_extracted_text.txt\"\n",
    "]\n",
    "\n",
    "# Tracking processed and missing courses\n",
    "processed_courses = []\n",
    "missing_files = {}\n",
    "\n",
    "# Check if each expected file exists\n",
    "for course in course_names:\n",
    "    missing_files[course] = []\n",
    "    for template in expected_file_templates:\n",
    "        file_name = template.format(course)\n",
    "        file_path = os.path.join(postgraduates_masters_dir, file_name)\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            processed_courses.append(file_name)\n",
    "        else:\n",
    "            missing_files[course].append(file_name)\n",
    "\n",
    "# Generate a verification report\n",
    "report_path = os.path.join(postgraduates_masters_dir, \"course_files_verification_report.txt\")\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as report_file:\n",
    "    report_file.write(\"### Processed Course Files ###\\n\")\n",
    "    report_file.write(\"\\n\".join(processed_courses) + \"\\n\\n\")\n",
    "\n",
    "    report_file.write(\"### Missing Course Files ###\\n\")\n",
    "    for course, missing in missing_files.items():\n",
    "        if missing:\n",
    "            report_file.write(f\"{course}:\\n\" + \"\\n\".join(missing) + \"\\n\\n\")\n",
    "\n",
    "print(f\"Verification report saved: {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# URL to check\n",
    "target_url = \"https://www.novaims.unl.pt/en/education/programs/postgraduate-programs-and-master-degree-programs/master-degree-in-geographic-information-systems-and-science-with-a-specialization-in-geographic-information-systems-and-science/\"\n",
    "\n",
    "# Check if the URL exists in the dictionary\n",
    "url_found = any(target_url in links for links in dict_programs_links.values())\n",
    "\n",
    "# Output the result\n",
    "if url_found:\n",
    "    print(f\"✅ The URL exists in dict_programs_links.\")\n",
    "else:\n",
    "    print(f\"❌ The URL was NOT found in dict_programs_links.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Setup WebDriver\n",
    "driver_path = ChromeDriverManager().install()\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# URL for the Data Science program\n",
    "data_science_url = \"https://www.novaims.unl.pt/en/education/programs/bachelor-s-degrees/data-science/\"\n",
    "\n",
    "# Open webpage\n",
    "driver.get(data_science_url)\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Extract the <main> element\n",
    "main_element = driver.find_element(By.TAG_NAME, \"main\")\n",
    "main_html = main_element.get_attribute('outerHTML')\n",
    "\n",
    "# Parse the page with BeautifulSoup\n",
    "soup = BeautifulSoup(main_html, \"html.parser\")\n",
    "\n",
    "\n",
    "# Remove unwanted sections\n",
    "unwanted_sections = soup.find_all(\"div\", class_=\"block\")\n",
    "if len(unwanted_sections) >= 4:\n",
    "    unwanted_sections[3].decompose()  # Remove only the fourth occurrence'\n",
    "    \n",
    "\n",
    "if len(unwanted_sections) >= 6:\n",
    "    unwanted_sections[5].decompose()  # Remove the sixth occurrence'\n",
    "    \n",
    "if len(unwanted_sections) >= 7:\n",
    "    unwanted_sections[6].decompose()  # Remove the seventh occurrence\n",
    "\n",
    "\n",
    "unwanted_section = soup.find(\"div\", class_=\"hero-article__info\")\n",
    "if unwanted_section:\n",
    "    unwanted_section.decompose()\n",
    "\n",
    "# Remove the specified articles\n",
    "articles = soup.find_all(\"article\", class_=\"content__article\")\n",
    "indices_to_remove = [5, 7, 8]  # 0-based index for 6th, 8th, and 9th articles\n",
    "for index in indices_to_remove:\n",
    "    if index < len(articles):\n",
    "        articles[index].decompose()\n",
    "\n",
    "# Extract only the text, removing extra whitespace\n",
    "main_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "# Print extracted text\n",
    "print(\"Extracted Text from Data Science Main Course Page:\")\n",
    "print(main_text)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Setup WebDriver\n",
    "driver_path = ChromeDriverManager().install()\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Target URL\n",
    "url = \"https://www.novaims.unl.pt/pt/ensino/cursos/licenciaturas/ciencia-de-dados/\"\n",
    "\n",
    "# Open webpage\n",
    "driver.get(url)\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Extract the <main> element\n",
    "main_element = driver.find_element(By.TAG_NAME, \"main\")\n",
    "\n",
    "# Get the inner HTML of <main>\n",
    "main_html = main_element.get_attribute('outerHTML')\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Parse with BeautifulSoup\n",
    "soup = BeautifulSoup(main_html, \"html.parser\")\n",
    "\n",
    "# Extract only the text, removing extra whitespace\n",
    "main_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "# Print extracted text\n",
    "print(main_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Setup WebDriver\n",
    "driver_path = ChromeDriverManager().install()\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Target URL\n",
    "url = \"https://www.novaims.unl.pt/pt/ensino/cursos/licenciaturas/ciencia-de-dados/\"\n",
    "\n",
    "# Open webpage\n",
    "driver.get(url)\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Extract the <main> element\n",
    "main_element = driver.find_element(By.TAG_NAME, \"main\")\n",
    "\n",
    "# Get the inner HTML of <main>\n",
    "main_html = main_element.get_attribute('outerHTML')\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Parse with BeautifulSoup\n",
    "soup = BeautifulSoup(main_html, \"html.parser\")\n",
    "\n",
    "# Remove the unwanted section\n",
    "unwanted_section = soup.find(\"div\", class_=\"hero-article__info\")\n",
    "if unwanted_section:\n",
    "    unwanted_section.decompose()  # Removes the element from the soup\n",
    "    \n",
    "\n",
    "# Extract only the text, removing extra whitespace\n",
    "main_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "# Print extracted text\n",
    "print(main_text)\n",
    "\n",
    "# Optionally, save to a text file\n",
    "with open(\"ciencia_de_dados_filtered_text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(main_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Setup WebDriver\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no UI)\n",
    "driver_path = ChromeDriverManager().install()\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Target URL\n",
    "url = \"https://www.novaims.unl.pt/pt/ensino/cursos/licenciaturas/ciencia-de-dados/\"\n",
    "\n",
    "# Open webpage\n",
    "driver.get(url)\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Scroll to ensure content is fully loaded\n",
    "driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "time.sleep(3)  # Allow content to load\n",
    "\n",
    "# Extract elements with the desired class\n",
    "elements = driver.find_elements(By.CLASS_NAME, \"slider-quote\")\n",
    "\n",
    "# Get text content from each element\n",
    "extracted_texts = [element.text.strip() for element in elements if element.text.strip()]\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Print extracted text\n",
    "for text in extracted_texts:\n",
    "    print(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Setup WebDriver\n",
    "driver_path = ChromeDriverManager().install()\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Target URL\n",
    "url = \"https://www.novaims.unl.pt/pt/ensino/cursos/licenciaturas/ciencia-de-dados/\"\n",
    "\n",
    "# Open webpage\n",
    "driver.get(url)\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Extract the <main> element\n",
    "main_element = driver.find_element(By.TAG_NAME, \"main\")\n",
    "\n",
    "# Get the inner HTML of <main>\n",
    "main_html = main_element.get_attribute('outerHTML')\n",
    "\n",
    "# Parse the original page with BeautifulSoup\n",
    "soup = BeautifulSoup(main_html, \"html.parser\")\n",
    "\n",
    "# Find the button link dynamically\n",
    "button = soup.find(\"a\", class_=\"content__link arrow-animate\")\n",
    "if button:\n",
    "    new_url = button['href']\n",
    "    if not new_url.startswith(\"http\"):\n",
    "        new_url = \"https://www.novaims.unl.pt\" + new_url\n",
    "\n",
    "    # Navigate to the new URL\n",
    "    driver.get(new_url)\n",
    "    time.sleep(5)  # Wait for the new page to load\n",
    "\n",
    "    # Extract the new page's <main> element\n",
    "    new_main_element = driver.find_element(By.TAG_NAME, \"main\")\n",
    "    new_main_html = new_main_element.get_attribute('outerHTML')\n",
    "\n",
    "    # Parse the new page with BeautifulSoup\n",
    "    new_soup = BeautifulSoup(new_main_html, \"html.parser\")\n",
    "\n",
    "    # Extract only the text from the new page, removing extra whitespace\n",
    "    new_main_text = new_soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # Print extracted text from the new page\n",
    "    print(\"\\nNew Page Text:\")\n",
    "    print(new_main_text)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Remove unwanted sections\n",
    "unwanted_sections = soup.find_all(\"div\", class_=\"block\")\n",
    "if len(unwanted_sections) >= 4:\n",
    "    unwanted_sections[3].decompose()  # Remove only the fourth occurrence\n",
    "if len(unwanted_sections) >= 6:\n",
    "    unwanted_sections[5].decompose()  # Remove the sixth occurrence\n",
    "if len(unwanted_sections) >= 7:\n",
    "    unwanted_sections[6].decompose()  # Remove the seventh occurrence\n",
    "\n",
    "unwanted_section = soup.find(\"div\", class_=\"hero-article__info\")\n",
    "if unwanted_section:\n",
    "    unwanted_section.decompose()\n",
    "\n",
    "# Extract only the text, removing extra whitespace\n",
    "main_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "# Print extracted text from the original page\n",
    "print(\"Original Page Text:\")\n",
    "print(main_text)\n",
    "\n",
    "# Optionally, save to a text file\n",
    "with open(\"ciencia_de_dados_filtered_text_3.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(main_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting every link inside div class=\"block has-aside\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Setup WebDriver\n",
    "driver_path = ChromeDriverManager().install()\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Target URL\n",
    "url = \"https://www.novaims.unl.pt/pt/ensino/cursos/licenciaturas/ciencia-de-dados/\"\n",
    "\n",
    "# Open webpage\n",
    "driver.get(url)\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Extract the <main> element\n",
    "main_element = driver.find_element(By.TAG_NAME, \"main\")\n",
    "\n",
    "# Get the inner HTML of <main>\n",
    "main_html = main_element.get_attribute('outerHTML')\n",
    "\n",
    "# Parse the original page with BeautifulSoup\n",
    "soup = BeautifulSoup(main_html, \"html.parser\")\n",
    "\n",
    "# Find all links inside the div with class \"block has-aside\"\n",
    "block_div = soup.find(\"div\", class_=\"block has-aside\")\n",
    "links = block_div.find_all(\"a\", href=True)\n",
    "\n",
    "# Print all found links\n",
    "for link in links:\n",
    "    print(link['href'])\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Setup WebDriver\n",
    "driver_path = ChromeDriverManager().install()\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Target URL\n",
    "url = \"https://www.novaims.unl.pt/pt/ensino/cursos/licenciaturas/ciencia-de-dados/\"\n",
    "\n",
    "# Open webpage\n",
    "driver.get(url)\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Extract the <main> element\n",
    "main_element = driver.find_element(By.TAG_NAME, \"main\")\n",
    "\n",
    "# Get the inner HTML of <main>\n",
    "main_html = main_element.get_attribute('outerHTML')\n",
    "\n",
    "# Parse the original page with BeautifulSoup\n",
    "soup = BeautifulSoup(main_html, \"html.parser\")\n",
    "\n",
    "# Find specific links\n",
    "links_to_extract = [\n",
    "    \"https://www.novaims.unl.pt/pt/ensino/cursos/licenciaturas/ciencia-de-dados/plano-de-estudos-ciencia-de-dados/#\",\n",
    "    \"https://www.novaims.unl.pt/pt/ensino/cursos/licenciaturas/ciencia-de-dados/corpo-docente/#\"\n",
    "]\n",
    "\n",
    "# Extract text from each specified link and save to different files\n",
    "for link in links_to_extract:\n",
    "    # Navigate to the new URL\n",
    "    driver.get(link)\n",
    "    time.sleep(5)  # Wait for the new page to load\n",
    "\n",
    "    # Extract the new page's <main> element\n",
    "    new_main_element = driver.find_element(By.TAG_NAME, \"main\")\n",
    "    new_main_html = new_main_element.get_attribute('outerHTML')\n",
    "\n",
    "    # Parse the new page with BeautifulSoup\n",
    "    new_soup = BeautifulSoup(new_main_html, \"html.parser\")\n",
    "\n",
    "    # Extract only the text from the new page, removing extra whitespace\n",
    "    new_main_text = new_soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # Determine the filename based on the link\n",
    "    if \"plano-de-estudos-ciencia-de-dados\" in link:\n",
    "        filename = \"plano_de_estudos_ciencia_de_dados.txt\"\n",
    "    elif \"corpo-docente\" in link:\n",
    "        filename = \"corpo_docente_ciencia_de_dados.txt\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Save the text content to a file\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(new_main_text)\n",
    "\n",
    "    # Print extracted text from the new page\n",
    "    print(f\"\\nText from {link}:\")\n",
    "    print(new_main_text)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
